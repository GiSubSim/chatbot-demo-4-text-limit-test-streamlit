import os
import sys
import json
from datetime import datetime
import streamlit as st
from dotenv import load_dotenv
import pandas as pd
from openai import OpenAI


# Streamlit ìŠ¤í¬ë¡¤ ë°©ì§€ìš© ì»´í¬ë„ŒíŠ¸
import streamlit.components.v1 as components 

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ëª¨ë¸ í™˜ê²½ ë³€ìˆ˜ ì½ê¸° ì¶”ê°€
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o")  # ê¸°ë³¸ê°’ gpt-4o

# ------------------------------
# í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ (ì™¸ë¶€ JSON)
# ------------------------------
PROMPTS_PATH = os.path.join(
    os.path.dirname(__file__),
    "prompts",
    "prompts.json"
)

# ------------------------------
# ê³ ì • ì§ˆë¬¸
# ------------------------------
RULE_QUESTIONS = {
    1: "ì¹œêµ¬ì•¼, ì˜¤ëŠ˜ ì–´ë• ì–´?",
    2: "ì˜¤ëŠ˜ í™œë™ ì¤‘ì— ê°€ì¥ ê¸°ì–µì— ë‚¨ì•˜ë˜ ìˆœê°„ì€ ë­ì˜€ì–´?",
    3: "ë§ˆì§€ë§‰ìœ¼ë¡œ, ì˜¤ëŠ˜ í™œë™ì„ ë§ˆì¹˜ë©° ë´‰ë´‰ì´ì—ê²Œ í•˜ê³  ì‹¶ì€ ë§ ìˆì„ê¹Œ?"
}

# ------------------------------
# ë‹¨ê³„ ë¼ë²¨ (í”„ë¡¬í”„íŠ¸ì— ë„£ëŠ” ì‚¬ëŒìš© ë¼ë²¨)
# ------------------------------
STAGE_LABELS = {
    1: "S1 í™œë™ë¬»ê¸° ë‹¨ê³„",
    2: "S2 ê¸°ì–µíšŒìƒ ë‹¨ê³„",
    3: "S3 í™œë™ ë§ˆë¬´ë¦¬ ë‹¨ê³„",
}

# ------------------------------
# ë””ë²„ê·¸ìš© í—¬í¼
# ------------------------------
def debug_block(title: str, lines: list[str]):
    """í„°ë¯¸ë„ì—ì„œ ë³´ê¸° ì¢‹ì€ ë””ë²„ê·¸ ë¸”ë¡ ì¶œë ¥."""
    print("\n" + "=" * 20 + f" {title} " + "=" * 20)
    for line in lines:
        print(line)
    print("=" * 60 + "\n")


# -------------------------------------------------
# í”„ë¡¬í”„íŠ¸ ìœ í‹¸ í•¨ìˆ˜ë“¤
# -------------------------------------------------
def load_prompts() -> dict:
    """
    prompts/prompts.json íŒŒì¼ì„ ì½ì–´ì„œ dictë¡œ ë°˜í™˜í•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜.
    - empathy_free_question
    - empathy_rule_question
    - empathy_ending_message
    ì„¸ ê°€ì§€ í‚¤ë¥¼ ê°€ì§„ JSON êµ¬ì¡°ë¥¼ ê¸°ëŒ€í•œë‹¤.
    """
    with open(PROMPTS_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)

    debug_block("LOAD PROMPTS", [
        f"PROMPTS_PATH: {PROMPTS_PATH}",
        f"keys: {list(data.keys())}"
    ])
    return data


def apply_prompt_template(lines, **kwargs) -> str:
    """
    prompts.jsonì—ì„œ ê°€ì ¸ì˜¨ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸(lines)ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê³ ,
    {{key}} í˜•íƒœì˜ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ kwargsë¡œ ì¹˜í™˜í•œë‹¤.
    """
    text = "\n".join(lines)
    for key, value in kwargs.items():
        placeholder = "{{" + key + "}}"
        text = text.replace(placeholder, value)
    return text


def extract_question_from_reply(reply: str) -> str:
    """
    GPT ì‘ë‹µ(reply)ì—ì„œ 'ì§ˆë¬¸ ë¬¸ì¥'ì„ ë‹¨ìˆœí•˜ê²Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ë³´ì¡° í•¨ìˆ˜.
    - ë§ˆì§€ë§‰ ì¤„ë¶€í„° ìœ„ë¡œ ì˜¬ë¼ê°€ë©°,
      ë¬¼ìŒí‘œ(?)ê°€ í¬í•¨ëœ ì²« ë²ˆì§¸ ë¹„ì–´ìˆì§€ ì•Šì€ ì¤„ì„ ì§ˆë¬¸ìœ¼ë¡œ ë³¸ë‹¤.
    - ì§ˆë¬¸ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´("")ì„ ë°˜í™˜.
    """
    lines = reply.splitlines()
    for line in reversed(lines):
        line = line.strip()
        if not line:
            continue
        if "?" in line:
            return line
    return ""


def build_fixed_questions_str() -> str:
    """
    RULE_QUESTIONS ì „ì²´ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í•œ ì¤„ ë¬¸ìì—´ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
    ì˜ˆ: "ì¹œêµ¬ì•¼, ì˜¤ëŠ˜ ì–´ë• ì–´? / ì˜¤ëŠ˜ í™œë™ ì¤‘ì— ê°€ì¥ ê¸°ì–µì— ë‚¨ì•˜ë˜ ìˆœê°„ì€ ë­ì˜€ì–´? / ..."
    """
    return " / ".join(RULE_QUESTIONS[i] for i in sorted(RULE_QUESTIONS.keys()))


def build_generated_questions_str() -> str:
    """
    ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ ììœ  ì§ˆë¬¸ ëª©ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜.
    - ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ 'í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ììœ  ì§ˆë¬¸ ì—†ìŒ'ìœ¼ë¡œ ë°˜í™˜.
    """
    generated = st.session_state.get("generated_questions", [])
    if not generated:
        return "í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ììœ  ì§ˆë¬¸ ì—†ìŒ"
    return " / ".join(generated)


# ------------------------------
# GPT FUNCTIONS (ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
# ------------------------------
def generate_empathy_free_question(user_message: str, stage: int, turn: int) -> str:
    """
    ê³µê° + ììœ  ì§ˆë¬¸ ìƒì„± (ê³ ì • ì§ˆë¬¸ ì‚¬ìš© X)
    - prompts.jsonì˜ empathy_free_question í…œí”Œë¦¿ ì‚¬ìš©
    - fixed_questions / generated_questions / stage_label / user_messageë¥¼ ì±„ì›Œì„œ ì „ë‹¬
    - ìƒì„±ëœ ì‘ë‹µì—ì„œ ë§ˆì§€ë§‰ 'ì§ˆë¬¸ ë¬¸ì¥'ì„ ì¶”ì¶œí•´ generated_questionsì— ëˆ„ì 
    """
    # ë‹¨ê³„ ë¼ë²¨ ì„¤ì •
    stage_label = STAGE_LABELS.get(stage, "ëŒ€í™” ë‹¨ê³„")

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ì„¸ì…˜ ìºì‹±)
    prompts = st.session_state["prompts"]
    lines = prompts["empathy_free_question"]

    # ê³ ì • ì§ˆë¬¸/ììœ  ì§ˆë¬¸ ëª©ë¡ ë¬¸ìì—´ ìƒì„±
    fixed_questions_str = build_fixed_questions_str()
    generated_questions_str = build_generated_questions_str()

    # í…œí”Œë¦¿ ì±„ìš°ê¸°
    prompt_text = apply_prompt_template(
        lines,
        stage_label=stage_label,
        user_message=user_message,
        fixed_questions=fixed_questions_str,
        generated_questions=generated_questions_str,
    )

    current_state = st.session_state.get("state")
    current_sub = st.session_state.get("substep")

    debug_block("GPT FREE QUESTION (empathy_free_question)", [
        f"[STATE] {current_state} / SUBSTEP {current_sub}",
        f"[STAGE_LABEL] {stage_label}",
        f"[TURN] {turn}",
        "",
        "[USER_MESSAGE]",
        user_message,
        "",
        "[FIXED_QUESTIONS_STR]",
        fixed_questions_str,
        "",
        "[GENERATED_QUESTIONS_STR]",
        generated_questions_str,
        "",
        "---------------- PROMPT TEXT SENT TO GPT ----------------",
        prompt_text
    ])

    # GPT í˜¸ì¶œ
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {
                "role": "system",
                "content": (
                    "ë„ˆëŠ” ì–´ë¦°ì´ë¥¼ ë”°ëœ»í•˜ê²Œ ë„ì™€ì£¼ëŠ” ìƒë‹´ ì±—ë´‡ 'ë´‰ë´‰'ì´ì•¼. "
                    "ì¹œê·¼í•œ ë°˜ë§ë¡œ ë§í•˜ê³ , ì•„ì´ì˜ ê°ì •ì„ ì¡´ì¤‘í•˜ë©´ì„œ ë¶€ë“œëŸ½ê²Œ ë°˜ì‘í•´."
                ),
            },
            {"role": "user", "content": prompt_text},
        ],
    )

    reply = response.choices[0].message.content

    # ìƒì„±ëœ ì‘ë‹µì—ì„œ ì§ˆë¬¸ ë¬¸ì¥ì„ ì¶”ì¶œí•´ ììœ  ì§ˆë¬¸ ëª©ë¡ì— ëˆ„ì 
    question_line = extract_question_from_reply(reply)
    already_exists = False

    if question_line:
        if "generated_questions" not in st.session_state:
            st.session_state["generated_questions"] = []
        if question_line in st.session_state["generated_questions"]:
            already_exists = True
        st.session_state["generated_questions"].append(question_line)

    debug_block("GPT FREE QUESTION RESULT", [
        "---------------- GPT RAW RESPONSE ----------------",
        reply,
        "",
        "-------------- EXTRACTED QUESTION -----------------",
        f"EXTRACTED: {repr(question_line)}",
        f"ALREADY_EXISTS: {already_exists}",
        "",
        "----------- UPDATED GENERATED_QUESTIONS ----------",
        build_generated_questions_str()
    ])

    return reply



def generate_empathy_rule_question(prev_answer: str, stage: int, rule_question: str) -> str:
    """
    ë‹¨ê³„ ì‹œì‘ ì‹œ: ì§ì „ ë‹µë³€ ê³µê° + (í˜„ì¬ í„´ì—ì„œ ì‚¬ìš©í• ) ê³ ì • ì§ˆë¬¸ 1ê°œ í¬í•¨í•´ì„œ ë¬»ëŠ” í•¨ìˆ˜.
    - prompts.jsonì˜ empathy_rule_question í…œí”Œë¦¿ ì‚¬ìš©
    - fixed_questions / generated_questions / stage_label / prev_answer / rule_question ì±„ì›Œì„œ ì „ë‹¬
    """
    stage_label = STAGE_LABELS.get(stage, "ë‹¤ìŒ ë‹¨ê³„")

    prompts = st.session_state["prompts"]
    lines = prompts["empathy_rule_question"]

    fixed_questions_str = build_fixed_questions_str()
    generated_questions_str = build_generated_questions_str()

    prompt_text = apply_prompt_template(
        lines,
        stage_label=stage_label,
        prev_answer=prev_answer,
        rule_question=rule_question,
        fixed_questions=fixed_questions_str,
        generated_questions=generated_questions_str,
    )

    current_state = st.session_state.get("state")
    current_sub = st.session_state.get("substep")

    debug_block("GPT RULE QUESTION (empathy_rule_question)", [
        f"[STATE] {current_state} / SUBSTEP {current_sub}",
        f"[STAGE_LABEL] {stage_label}",
        "",
        "[PREV_ANSWER]",
        prev_answer,
        "",
        "[RULE_QUESTION]",
        rule_question,
        "",
        "[FIXED_QUESTIONS_STR]",
        fixed_questions_str,
        "",
        "[GENERATED_QUESTIONS_STR]",
        generated_questions_str,
        "",
        "---------------- PROMPT TEXT SENT TO GPT ----------------",
        prompt_text
    ])


    # GPT í˜¸ì¶œ
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {
                "role": "system",
                "content": (
                    "ë„ˆëŠ” ì–´ë¦°ì´ë¥¼ ë”°ëœ»í•˜ê²Œ ë„ì™€ì£¼ëŠ” ìƒë‹´ ì±—ë´‡ 'ë´‰ë´‰'ì´ì•¼. "
                    "ì¹œê·¼í•œ ë°˜ë§ë¡œ ë§í•˜ê³ , ì•„ì´ì˜ ë§ì„ ë¨¼ì € ê³µê°í•´ ì¤€ ë’¤, "
                    "ì´ë²ˆ í„´ì—ì„œ ì‚¬ìš©í•  ê³ ì • ì§ˆë¬¸ì„ ìì—°ìŠ¤ëŸ½ê²Œ í•œ ë²ˆë§Œ ì‚¬ìš©í•´ì•¼ í•´."
                ),
            },
            {"role": "user", "content": prompt_text},
        ],
    )

    reply = response.choices[0].message.content

    debug_block("GPT RULE QUESTION RESULT", [
        "---------------- GPT RAW RESPONSE ----------------",
        reply
    ])

    return reply



def generate_empathy_ending_message(user_message: str) -> str:
    """
    S3 ë§ˆì§€ë§‰ GPT í„´ â€” ê³µê° + ë§ˆë¬´ë¦¬ ë©”ì‹œì§€ ìƒì„±.
    - ì§ˆë¬¸ ì—†ì´ ëë‚˜ì•¼ í•˜ë©°, ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë°˜ë“œì‹œ 'ì•ˆë…•'ìœ¼ë¡œ ëë‚˜ì•¼ í•¨.
    - prompts.jsonì˜ empathy_ending_message í…œí”Œë¦¿ ì‚¬ìš©.
    """
    prompts = st.session_state["prompts"]
    lines = prompts["empathy_ending_message"]

    fixed_questions_str = build_fixed_questions_str()
    generated_questions_str = build_generated_questions_str()

    prompt_text = apply_prompt_template(
        lines,
        user_message=user_message,
        fixed_questions=fixed_questions_str,
        generated_questions=generated_questions_str,
    )

    current_state = st.session_state.get("state")
    current_sub = st.session_state.get("substep")

    debug_block("GPT ENDING MESSAGE (empathy_ending_message)", [
        f"[STATE] {current_state} / SUBSTEP {current_sub}",
        "",
        "[USER_MESSAGE]",
        user_message,
        "",
        "[FIXED_QUESTIONS_STR]",
        fixed_questions_str,
        "",
        "[GENERATED_QUESTIONS_STR]",
        generated_questions_str,
        "",
        "---------------- PROMPT TEXT SENT TO GPT ----------------",
        prompt_text
    ])

    # GPT í˜¸ì¶œ
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {
                "role": "system",
                "content": (
                    "ë„ˆëŠ” ì˜¤ëŠ˜ í™œë™ì„ ë§ˆë¬´ë¦¬í•˜ëŠ” ë§ˆì§€ë§‰ ì¸ì‚¬ë¥¼ í•˜ëŠ” ìƒë‹´ ì±—ë´‡ 'ë´‰ë´‰'ì´ì•¼. "
                    "ì ˆëŒ€ ì§ˆë¬¸ì„ í•˜ì§€ ë§ê³ , ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë°˜ë“œì‹œ 'ì•ˆë…•'ìœ¼ë¡œ ëë‚´ì•¼ í•´."
                ),
            },
            {"role": "user", "content": prompt_text},
        ],
    )


    reply = response.choices[0].message.content

    debug_block("GPT ENDING MESSAGE RESULT", [
        "---------------- GPT RAW RESPONSE ----------------",
        reply
    ])

    return reply



# -------------------------------------------------
# ì„¸ì…˜ ì´ˆê¸°í™”
# -------------------------------------------------
def init_session():
    first_init = "messages" not in st.session_state

    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    if "state" not in st.session_state:
        st.session_state["state"] = 1  # 1=S1, 2=S2, 3=S3

    if "substep" not in st.session_state:
        st.session_state["substep"] = 1  # 1~6

    if "downloads_enabled" not in st.session_state:
        st.session_state["downloads_enabled"] = False

    # ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ ììœ  ì§ˆë¬¸ ëª©ë¡ (ì¤‘ë³µ ì§ˆë¬¸ ë°©ì§€ìš©)
    if "generated_questions" not in st.session_state:
        st.session_state["generated_questions"] = []

    # â­ prompts.json íŒŒì¼ì„ ìµœì´ˆ 1ë²ˆë§Œ ì½ì–´ ìºì‹±
    if "prompts" not in st.session_state:
        st.session_state["prompts"] = load_prompts()

    debug_block("INIT SESSION", [
        f"FIRST_INIT: {first_init}",
        f"state: {st.session_state['state']}",
        f"substep: {st.session_state['substep']}",
        f"downloads_enabled: {st.session_state['downloads_enabled']}",
        f"generated_questions: {st.session_state['generated_questions']}",
        f"prompts_loaded_keys: {list(st.session_state['prompts'].keys())}"
    ])


# -------------------------------------------------
# í„´ ë‹¨ìœ„ íŒŒì¼ ì €ì¥ (ì‹¤ì‹œê°„ append) - ë°ëª¨ 1ì—ì„œ ê°€ì ¸ì˜´
# -------------------------------------------------
def append_turn_to_file(role, text):
    current_index = len(st.session_state["messages"]) - 1
    turn_number = (current_index // 2) + 1
    
    log = {
        "session_id": "sess_004",  # ì„¸ì…˜ IDëŠ” ë°ëª¨ 4ë¥¼ ë°˜ì˜í•˜ì—¬ ë³€ê²½
        "timestamp": datetime.now().isoformat(),
        "role": role,
        "text": text,
        "turn": turn_number
    }
    
    # ì €ì¥ ê²½ë¡œ ì„¤ì •
    log_dir = os.path.join(os.path.dirname(__file__), "data/logs")  # ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ê²½ë¡œ ì¡°ì •
    os.makedirs(log_dir, exist_ok=True)

    log_path = os.path.join(log_dir, "chat_log.jsonl")

    # JSONL í˜•ì‹ ì €ì¥
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(log, ensure_ascii=False) + "\n")

    print(f"[FILE_APPEND] role={role}, turn={turn_number}, path={log_path}")


# -------------------------------------------------
# ë©”ì‹œì§€ ì¶”ê°€ (ë¡œê·¸ ì €ì¥ í†µí•©)
# -------------------------------------------------
def add_message(role: str, text: str):
    st.session_state["messages"].append({
        "role": role,
        "message": text,
        "timestamp": datetime.now().isoformat()
    })
    
    current_index = len(st.session_state["messages"]) - 1
    turn_number = (current_index // 2) + 1

    debug_block("ADD MESSAGE", [
        f"ROLE: {role}",
        f"TEXT: {text}",
        f"MESSAGES_LEN: {len(st.session_state['messages'])}",
        f"TURN_NUMBER(approx): {turn_number}"
    ])
    
    # í„´ ë‹¨ìœ„ íŒŒì¼ ì‹¤ì‹œê°„ ì €ì¥
    append_turn_to_file(role, text)


# -------------------------------------------------
# UI ë Œë”ë§
# -------------------------------------------------
def render_chat_messages():
    for msg in st.session_state["messages"]:
        if msg["role"] == "bot":
            st.markdown(f"""
            <div style="text-align:left;">
                <div style="
                    display:inline-block; background:#f1f0f0;
                    padding:12px 15px; border-radius:12px;
                    margin:5px 0; max-width:70%;
                    font-size:16px;
                    color:#000000;">
                    ğŸ§¸ <b>ë´‰ë´‰</b><br>{msg['message']}
                </div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div style="text-align:right;">
                <div style="
                    display:inline-block; background:#d1e7ff;
                    padding:12px 15px; border-radius:12px;
                    margin:5px 0; max-width:70%;
                    font-size:16px;
                    color:#000000;">
                    ğŸŒŸ <b>ë‚˜</b><br>{msg['message']}
                </div>
            </div>
            """, unsafe_allow_html=True)


# -------------------------------------------------
# CSV ì €ì¥ í•¨ìˆ˜ (ë°ëª¨ 1ì—ì„œ ê°€ì ¸ì˜´)
# -------------------------------------------------
def save_as_csv(disabled: bool = False):
    msgs = st.session_state["messages"]
    if not msgs:
        st.download_button("â¬‡ CSV ë‹¤ìš´ë¡œë“œ", data="", file_name="chat_turns.csv", mime="text/csv", disabled=True)
        return

    session_id = "sess_004"
    user_id = "user_abc123"
    created_at = datetime.now().isoformat()
    chat_type = "fsm_empathy_2turn"

    rows = []
    turn = 1

    for i in range(0, len(msgs), 2):
        bot_msg = msgs[i] if msgs[i]["role"] == "bot" else None
        user_msg = msgs[i+1] if i+1 < len(msgs) and msgs[i+1]["role"] == "user" else None

        if bot_msg:
            rows.append({
                "session_id": session_id, "user_id": user_id, "created_at": created_at,
                "chat_type": chat_type, "turn": turn, "role": "bot",
                "text": bot_msg["message"], "timestamp": bot_msg["timestamp"],
            })

        if user_msg:
            rows.append({
                "session_id": session_id, "user_id": user_id, "created_at": created_at,
                "chat_type": chat_type, "turn": turn, "role": "user",
                "text": user_msg["message"], "timestamp": user_msg["timestamp"],
            })
        elif bot_msg:
            rows.append({
                "session_id": session_id, "user_id": user_id, "created_at": created_at,
                "chat_type": chat_type, "turn": turn, "role": "user",
                "text": "", "timestamp": "",
            })

        turn += 1

    df = pd.DataFrame(rows)
    csv_bytes = df.to_csv(index=False).encode("utf-8-sig")

    st.download_button(
        "â¬‡ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv_bytes,
        file_name="chat_turns.csv",
        mime="text/csv",
        disabled=disabled
    )


# -------------------------------------------------
# JSON ì €ì¥ í•¨ìˆ˜ (ë°ëª¨ 1ì—ì„œ ê°€ì ¸ì˜´)
# -------------------------------------------------
def save_as_json(disabled: bool = False):
    msgs = st.session_state["messages"]
    if not msgs:
        st.download_button("â¬‡ JSON ë‹¤ìš´ë¡œë“œ", data="", file_name="chat_history.json", mime="application/json", disabled=True)
        return

    dialogue = []
    turn_index = 1

    for i in range(0, len(msgs), 2):
        bot_msg = msgs[i] if i < len(msgs) and msgs[i]["role"] == "bot" else None
        user_msg = msgs[i+1] if (i+1) < len(msgs) and msgs[i+1]["role"] == "user" else None

        bot_block = {"role": "bot", "text": bot_msg["message"], "timestamp": bot_msg["timestamp"]} if bot_msg else None
        user_block = {"role": "user", "text": user_msg["message"], "timestamp": user_msg["timestamp"]} if user_msg else None

        dialogue.append({
            "turn": turn_index,
            "bot": bot_block,
            "user": user_block
        })
        turn_index += 1

    data = {
        "session_id": "sess_004",
        "user_id": "user_abc123",
        "created_at": datetime.now().isoformat(),
        "chat_type": "fsm_empathy_2turn",
        "dialogue": dialogue
    }

    json_str = json.dumps(data, ensure_ascii=False, indent=2)

    st.download_button(
        "â¬‡ JSON ë‹¤ìš´ë¡œë“œ",
        json_str,
        file_name="chat_history.json",
        mime="application/json",
        disabled=disabled
    )


# -------------------------------------------------
# Main FSM Process (ì•ˆì •í™”ëœ ìµœì¢… êµ¬ì¡°)
# -------------------------------------------------
def process_flow(user_input=None):
    """
    process_flow(user_input)

    ì±—ë´‡ì˜ ì „ì²´ ëŒ€í™” ë‹¨ê³„ë¥¼ ê´€ë¦¬í•˜ëŠ” Finite State Machine(FSM).
    """

    debug_block("PROCESS FLOW - ENTER", [
        f"RAW user_input: {repr(user_input)}",
        f"CURRENT state: {st.session_state.get('state')}",
        f"CURRENT substep: {st.session_state.get('substep')}"
    ])
    
    # ğŸ”¥ 0. ëŒ€í™” ì¢…ë£Œ í›„ ì…ë ¥/ìë™ì§„í–‰ ì™„ì „ ì°¨ë‹¨
    if st.session_state["state"] == 3 and st.session_state["substep"] == 6:
        debug_block("PROCESS FLOW - END STATE", [
            "state=3 & substep=6 â†’ ì¢…ë£Œ ìƒíƒœ, ì¶”ê°€ ì²˜ë¦¬ ì—†ìŒ"
        ])
        return

    state = st.session_state["state"]
    sub = st.session_state["substep"]

    # GPT ë‹µë³€ ì¤‘(sub=1,3,5)ì— ë“¤ì–´ì˜¨ ìœ ì € ì…ë ¥ì€ ë¬´ì‹œ
    if user_input and sub not in [2, 4, 6]:
        debug_block("PROCESS FLOW - IGNORE USER INPUT", [
            f"sub={sub} (GPT ìë™ ë°œí™” í„´) ì´ë¯€ë¡œ, user_input ë¬´ì‹œ"
        ])
        return

    # -------------------------------------------------
    # 1. ìœ ì € ì…ë ¥ ì²˜ë¦¬ (sub 2, 4, 6)
    # -------------------------------------------------
    if user_input:
        debug_block("PROCESS FLOW - USER INPUT HANDLING", [
            f"state={state}, substep={sub}",
            f"user_input: {user_input}"
        ])

        add_message("user", user_input)

        if sub in [2, 4]:
            st.session_state["substep"] += 1
            debug_block("PROCESS FLOW - MOVE TO NEXT GPT TURN", [
                f"NEXT substep: {st.session_state['substep']}"
            ])
            st.rerun()

        elif sub == 6:
            if state < 3:
                st.session_state["state"] += 1
                st.session_state["substep"] = 1
                debug_block("PROCESS FLOW - MOVE TO NEXT STATE", [
                    f"NEXT state: {st.session_state['state']}",
                    f"RESET substep: {st.session_state['substep']}"
                ])
                st.rerun()
            else:
                debug_block("PROCESS FLOW - FINAL USER INPUT AT END", [
                    "state=3 & substep=6 ì—ì„œ user_input ì²˜ë¦¬ í›„ ì¢…ë£Œ"
                ])
                return

        return  # user_input ì²˜ë¦¬ ì¢…ë£Œ

    # -------------------------------------------------
    # 2. GPT/RULE ìë™ ë°œí™” ì²˜ë¦¬ (user_input == Noneì¼ ë•Œ)
    # -------------------------------------------------
    
    # S1 í™œë™ë¬»ê¸°
    if state == 1:
        if sub == 1:  # S1-1: ë£°ë² ì´ìŠ¤ ê³ ì • ì§ˆë¬¸ (ì²« ë¡œë”© ì‹œì )
            debug_block("FSM AUTO BOT - S1 SUB1", [
                "RULE_QUESTION[1] ë°œí™”"
            ])
            add_message("bot", RULE_QUESTIONS[1])
            st.session_state["substep"] = 2
            debug_block("FSM TRANSITION", [
                "state=1 ìœ ì§€, substep 1 â†’ 2"
            ])
            st.rerun()
        
        if sub == 3:  # S1-3: GPT ê³µê° 1í„´
            last = st.session_state["messages"][-1]["message"]
            debug_block("FSM AUTO BOT - S1 SUB3", [
                f"LAST USER MSG: {last}"
            ])
            bot_msg = generate_empathy_free_question(last, 1, 1)
            add_message("bot", bot_msg)
            st.session_state["substep"] = 4
            debug_block("FSM TRANSITION", [
                "state=1 ìœ ì§€, substep 3 â†’ 4"
            ])
            st.rerun()
        
        if sub == 5:  # S1-5: GPT ê³µê° 2í„´
            last = st.session_state["messages"][-1]["message"]
            debug_block("FSM AUTO BOT - S1 SUB5", [
                f"LAST USER MSG: {last}"
            ])
            bot_msg = generate_empathy_free_question(last, 1, 2)
            add_message("bot", bot_msg)
            st.session_state["substep"] = 6
            debug_block("FSM TRANSITION", [
                "state=1 ìœ ì§€, substep 5 â†’ 6"
            ])
            st.rerun()

    # S2 ê¸°ì–µíšŒìƒ
    elif state == 2:
        if sub == 1:  # S2-1: GPT ê³µê° + ê³ ì • ì§ˆë¬¸ (S1 ì¢…ë£Œ í›„)
            prev_answer = st.session_state["messages"][-1]["message"]
            fixed = RULE_QUESTIONS[2]
            debug_block("FSM AUTO BOT - S2 SUB1", [
                f"PREV_ANSWER: {prev_answer}",
                f"FIXED_QUESTION: {fixed}"
            ])
            bot_msg = generate_empathy_rule_question(prev_answer, 2, fixed)
            add_message("bot", bot_msg)
            st.session_state["substep"] = 2
            debug_block("FSM TRANSITION", [
                "state=2 ìœ ì§€, substep 1 â†’ 2"
            ])
            st.rerun()
            
        if sub == 3:  # S2-3: GPT ê³µê° 1í„´
            last = st.session_state["messages"][-1]["message"]
            debug_block("FSM AUTO BOT - S2 SUB3", [
                f"LAST USER MSG: {last}"
            ])
            bot_msg = generate_empathy_free_question(last, 2, 1)
            add_message("bot", bot_msg)
            st.session_state["substep"] = 4
            debug_block("FSM TRANSITION", [
                "state=2 ìœ ì§€, substep 3 â†’ 4"
            ])
            st.rerun()
            
        if sub == 5:  # S2-5: GPT ê³µê° 2í„´
            last = st.session_state["messages"][-1]["message"]
            debug_block("FSM AUTO BOT - S2 SUB5", [
                f"LAST USER MSG: {last}"
            ])
            bot_msg = generate_empathy_free_question(last, 2, 2)
            add_message("bot", bot_msg)
            st.session_state["substep"] = 6
            debug_block("FSM TRANSITION", [
                "state=2 ìœ ì§€, substep 5 â†’ 6"
            ])
            st.rerun()

    # S3 ë§ˆë¬´ë¦¬
    elif state == 3:
        if sub == 1:  # S3-1: GPT ê³µê° + ê³ ì • ì§ˆë¬¸ (S2 ì¢…ë£Œ í›„)
            prev_answer = st.session_state["messages"][-1]["message"]
            fixed = RULE_QUESTIONS[3]
            debug_block("FSM AUTO BOT - S3 SUB1", [
                f"PREV_ANSWER: {prev_answer}",
                f"FIXED_QUESTION: {fixed}"
            ])
            bot_msg = generate_empathy_rule_question(prev_answer, 3, fixed)
            add_message("bot", bot_msg)
            st.session_state["substep"] = 2
            debug_block("FSM TRANSITION", [
                "state=3 ìœ ì§€, substep 1 â†’ 2"
            ])
            st.rerun()
            
        if sub == 3:  # S3-3: GPT ê³µê° 1í„´
            last = st.session_state["messages"][-1]["message"]
            debug_block("FSM AUTO BOT - S3 SUB3", [
                f"LAST USER MSG: {last}"
            ])
            bot_msg = generate_empathy_free_question(last, 3, 1)
            add_message("bot", bot_msg)
            st.session_state["substep"] = 4
            debug_block("FSM TRANSITION", [
                "state=3 ìœ ì§€, substep 3 â†’ 4"
            ])
            st.rerun()
            
        if sub == 5:  # S3-5: GPT ê³µê° 2í„´ (ë§ˆë¬´ë¦¬ ë°œí™”)
            last = st.session_state["messages"][-1]["message"]
            debug_block("FSM AUTO BOT - S3 SUB5 (ENDING)", [
                f"LAST USER MSG: {last}"
            ])
            bot_msg = generate_empathy_ending_message(last)
            add_message("bot", bot_msg)
            st.session_state["substep"] = 6
            debug_block("FSM TRANSITION", [
                "state=3 ìœ ì§€, substep 5 â†’ 6 (END STATE CANDIDATE)"
            ])
            st.rerun()


# -------------------------------------------------
# MAIN
# -------------------------------------------------
def main():
    st.set_page_config(layout="centered", page_title="Chatbot Demo â€“ Step 4")

    st.markdown("""
        <style>
        .top-right-info {
            position:absolute; top:10px; right:20px;
            font-size:14px; color:#999;
        }
        div[role="radiogroup"] > label { margin-bottom: 5px; }
        .stSuccess { text-align: center; }
        </style>
        <div class="top-right-info">
            (ë‹´ë‹¹ì: ë¯¸ìˆ ì¸ì§€ì‹¬ë¦¬ì—°êµ¬ì†Œ ì‹¬ê¸°ì„­)
        </div>
    """, unsafe_allow_html=True)

    # íƒ€ì´í‹€
    st.markdown("""
        <div style='text-align:center; margin-top: 20px; margin-bottom: 30px;'>
            <div style='font-size: 34px; font-weight: 700;'>ğŸ’› Chatbot Demo â€“ Step 4</div>
            <div style='font-size: 24px; font-weight: 500; margin-top: -5px;'>(í™œë™ ë§ˆë¬´ë¦¬ ì±—ë´‡)</div>
        </div>
    """, unsafe_allow_html=True)

    # ëª¨ë¸ëª… í‘œê¸°
    st.markdown(
        f"<div style='text-align:right; color:#888; font-size:14px;'>ğŸ”® model: {MODEL_NAME}</div>",
        unsafe_allow_html=True
    )

    init_session()

    # 1. ë Œë”ë§ (ì´ì „ ì„¸ì…˜ ìƒíƒœ)
    render_chat_messages()

    state = st.session_state["state"]
    sub = st.session_state["substep"]
    downloads_enabled = st.session_state.get("downloads_enabled", False)

    # --- ì…ë ¥ ê°€ëŠ¥ substep ì •ì˜: ìœ ì € ì…ë ¥ í„´ë§Œ ê°€ëŠ¥ ---
    can_user_input = (sub in [2, 4, 6])

    # --- S3 ì¢…ë£Œ(sub=6)ë©´ ì…ë ¥ì°½ ìˆ¨ê¹€ ---
    if state == 3 and sub == 6:
        can_user_input = False

    # í•­ìƒ ê¸°ë³¸ê°’ ë¨¼ì € ì„ ì–¸ (ì˜¤ë¥˜ ë°©ì§€)
    user_input = None  

    if can_user_input:
        raw_input = st.chat_input("ë´‰ë´‰ì—ê²Œ ë§ˆìŒì„ ì´ì•¼ê¸°í•´ì¤˜ ğŸ˜Š")

        if raw_input:  # ì…ë ¥ì´ ë“¤ì–´ì˜¨ ê²½ìš°
            # ê³µë°± ì œì™¸ ê¸°ì¤€ ê¸€ì ìˆ˜
            char_count = len(raw_input.replace(" ", "").replace("\n", ""))

            # ê°œë°œì í„°ë¯¸ë„ ë¡œê·¸
            print(f"[USER INPUT RECEIVED] length={char_count} chars (ê³µë°± ì œì™¸)")

            # 200ì ì´ˆê³¼ ì‹œ ìë™ ìë¥´ê¸°
            if char_count > 200:
                # ì•ì—ì„œë¶€í„° 200ê¸€ìë§Œ ë‚¨ê¹€
                trimmed = raw_input.replace(" ", "").replace("\n", "")[:200]
                user_input = trimmed
                print(f"[TRIMMED] Input exceeded 200 chars â†’ trimmed to 200.")
            else:
                user_input = raw_input

    else:
        user_input = None


    process_flow(user_input)
    
    # 4. ì €ì¥ ì˜ì—­ (S3-5 â†’ substep=6 ì´í›„ ì²˜ë¦¬)
    state = st.session_state["state"]
    sub = st.session_state["substep"]
    downloads_enabled = st.session_state.get("downloads_enabled", False)

    # (1) ì•„ì§ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ëˆ„ë¥´ê¸° ì „ â†’ ì¤‘ì•™ì— "ëŒ€í™” ì €ì¥" ë²„íŠ¼ë§Œ í‘œì‹œ
    if state == 3 and sub == 6 and not downloads_enabled:

        st.markdown("<div style='height:25px;'></div>", unsafe_allow_html=True)

        c1, c2, c3 = st.columns([1, 2, 1])
        with c2:
            save_click = st.button("ë§ˆë¬´ë¦¬ ì™„ë£Œ", use_container_width=True)

        if save_click:
            st.session_state["downloads_enabled"] = True
            debug_block("DOWNLOAD ENABLED", [
                "downloads_enabled set to True"
            ])
            st.rerun()

        st.markdown(
            "<div style='margin-top:35px; margin-bottom:20px; border-top:1px solid #666;'></div>",
            unsafe_allow_html=True
        )

    elif downloads_enabled:
        st.markdown("<hr style='margin-top:35px; margin-bottom:20px;'>", unsafe_allow_html=True)
        st.subheader("ğŸ“¥ ëŒ€í™” ì €ì¥")

        btn_area = st.columns([0.15, 0.15, 0.7])
        with btn_area[0]:
            save_as_json(disabled=False)
        with btn_area[1]:
            save_as_csv(disabled=False)

        js_code = """
        <script>
            var body = window.parent.document.querySelector('.main');
            body.scrollTop = body.scrollHeight;
        </script>
        """
        components.html(js_code, height=0)


if __name__ == "__main__":
    main()
